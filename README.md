# ğŸ’» SpectraPrice - Intelligent Price Prediction Engine

<div align="center">
  
  [![Made with Scikit-learn](https://img.shields.io/badge/Made%20with-Scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn)](https://scikit-learn.org)
  [![Powered by Streamlit](https://img.shields.io/badge/UI-Streamlit-FF4B4B?style=for-the-badge&logo=streamlit)](https://streamlit.io)
 
  [![Pandas](https://img.shields.io/badge/Data-Pandas-150458?style=for-the-badge&logo=pandas)](https://pandas.pydata.org)
  
  **An intelligent laptop price prediction system using ensemble machine learning models**
  
  *Predict laptop prices accurately based on 12 key specifications using advanced regression techniques!*
  
</div>

## ğŸŒŸ Features

<div align="center">
  
  | ğŸ¤– **Ensemble Learning** | ğŸ¯ **Smart Predictions** | ğŸ“Š **Comprehensive EDA** |
  |:---------------------:|:----------------------:|:--------------------------:|
  | Multiple regression models | High accuracy predictions | Deep data analysis |
  | Voting & Stacking methods | RÂ² score: 0.87+ | Visual insights |
  
  | ğŸ¨ **Interactive UI** | ğŸ”„ **Feature Engineering** | ğŸ“ˆ **Model Comparison** |
  |:----------------------:|:-----------------------:|:---------------------------:|
  | Streamlit web interface | 20+ engineered features | 12+ tested algorithms |
  | Real-time predictions | Smart preprocessing | Performance benchmarking |
  
</div>

## âœ¨ What makes this Predictor special?

- **ğŸ¤– Advanced Ensemble Learning** - Combines Random Forest, XGBoost, Gradient Boosting for superior accuracy
- **ğŸ¯ High Prediction Accuracy** - RÂ² score of 0.87+ with mean absolute error under 0.15
- **ğŸ“Š Extensive EDA** - Comprehensive exploratory data analysis with 15+ visualizations
- **ğŸ”„ Smart Feature Engineering** - Transforms raw specs into 20+ meaningful features
- **ğŸ¨ User-Friendly Interface** - Beautiful Streamlit web app for instant predictions
- **ğŸ’¾ Model Persistence** - Trained models saved for instant deployment
- **ğŸ“ˆ Multiple Algorithms** - Tested 12 different regression models for optimal performance
- **ğŸ”§ Production Ready** - Complete pipeline from data preprocessing to deployment

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.8 or higher
python --version

# pip package manager
pip --version
```

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/laptop-price-predictor.git
   cd laptop-price-predictor
   ```

2. **Install dependencies**
   ```bash
   pip install numpy pandas matplotlib seaborn
   pip install scikit-learn xgboost
   pip install streamlit
   pip install pickle-mixin
   ```

3. **Run the Streamlit app**
   ```bash
   streamlit run app.py
   ```

4. **Make predictions!**
   - Select laptop specifications from dropdowns
   - Click "Predict Price" button
   - Get instant price prediction in your currency

## ğŸ› ï¸ Tech Stack

<div align="center">
  
  ![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
  ![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
  ![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
  ![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
  ![Matplotlib](https://img.shields.io/badge/Matplotlib-11557C?style=for-the-badge&logo=python&logoColor=white)
  ![Seaborn](https://img.shields.io/badge/Seaborn-3776AB?style=for-the-badge&logo=python&logoColor=white)
  ![XGBoost](https://img.shields.io/badge/XGBoost-337AB7?style=for-the-badge&logo=xgboost&logoColor=white)
  
</div>

### Core Technologies

- **Machine Learning Framework**: Scikit-learn (Pipeline, preprocessing, and 10+ regression models)
- **Ensemble Methods**: XGBoost (Gradient boosting implementation)
- **Web Framework**: Streamlit (Interactive web application)
- **Data Processing**: Pandas & NumPy (Data manipulation and numerical computing)
- **Visualization**: Matplotlib & Seaborn (Exploratory data analysis)
- **Model Persistence**: Pickle (Model and data serialization)

## ğŸ’¡ How It Works

### ML Pipeline Architecture

```mermaid
graph TD
    A[ğŸ“ Raw Dataset] --> B[ğŸ§¹ Data Cleaning]
    B --> C[ğŸ” EDA & Visualization]
    C --> D[âš™ï¸ Feature Engineering]
    D --> E[ğŸ”„ Data Preprocessing]
    E --> F[ğŸ¤– Model Training]
    F --> G[ğŸ“Š Model Evaluation]
    G --> H[ğŸ† Best Model Selection]
    H --> I[ğŸ’¾ Model Serialization]
    I --> J[ğŸ¨ Streamlit Deployment]
    J --> K[ğŸ¯ Price Predictions]
    
    L[ğŸ“ˆ 12 Algorithms Tested] --> G
    M[ğŸ”„ Cross-Validation] --> G
    
    style A fill:#4ECDC4,stroke:#333,stroke-width:2px,color:#fff
    style F fill:#FF6B6B,stroke:#333,stroke-width:2px,color:#fff
    style H fill:#45B7D1,stroke:#333,stroke-width:2px,color:#fff
    style K fill:#96CEB4,stroke:#333,stroke-width:2px,color:#fff
```

### Prediction Process

1. **ğŸ“Š Data Collection** - User inputs laptop specifications through Streamlit interface
2. **ğŸ”„ Preprocessing** - Features are transformed using OneHotEncoding for categorical variables
3. **âš™ï¸ Feature Engineering** - Calculate PPI (pixels per inch) and process memory specifications
4. **ğŸ¤– Model Prediction** - Ensemble model predicts log-transformed price
5. **ğŸ¯ Price Output** - Exponential transformation applied to get final price prediction
6. **ğŸ“ˆ Confidence Display** - Prediction shown with user-friendly formatting

## ğŸ® Dataset Overview

### ğŸ“ Data Specifications

- **Total Samples**: 1,303 laptops
- **Features**: 12 original columns + 20+ engineered features
- **Target Variable**: Price (continuous variable)
- **Data Source**: `laptop_data.csv`

### ğŸ“Š Key Features

```python
Original Features:
â”œâ”€â”€ Company (Categorical) - 12 brands
â”œâ”€â”€ TypeName (Categorical) - 6 laptop types
â”œâ”€â”€ Inches (Numerical) - Screen size
â”œâ”€â”€ ScreenResolution (Text) - Display specifications
â”œâ”€â”€ Cpu (Text) - Processor details
â”œâ”€â”€ Ram (Text) - Memory in GB
â”œâ”€â”€ Memory (Text) - Storage configuration
â”œâ”€â”€ Gpu (Text) - Graphics card
â”œâ”€â”€ OpSys (Categorical) - Operating system
â”œâ”€â”€ Weight (Text) - Laptop weight in kg
â””â”€â”€ Price (Target) - Price in currency units

Engineered Features:
â”œâ”€â”€ Touchscreen (Binary) - Touchscreen capability
â”œâ”€â”€ IPS (Binary) - IPS display technology
â”œâ”€â”€ X_res (Numerical) - Horizontal resolution
â”œâ”€â”€ Y_res (Numerical) - Vertical resolution
â”œâ”€â”€ ppi (Numerical) - Pixels per inch
â”œâ”€â”€ Cpu brand (Categorical) - Processor brand
â”œâ”€â”€ HDD (Numerical) - HDD storage in GB
â”œâ”€â”€ SSD (Numerical) - SSD storage in GB
â”œâ”€â”€ Hybrid (Numerical) - Hybrid storage
â”œâ”€â”€ Flash_Storage (Numerical) - Flash storage
â”œâ”€â”€ Gpu brand (Categorical) - GPU manufacturer
â””â”€â”€ os (Categorical) - Categorized OS
```



## ğŸ“Š Exploratory Data Analysis Insights

### Key Findings

#### ğŸ’° Price Distribution
- Right-skewed distribution with most laptops in lower price range
- Few high-end models significantly increase average price
- Log transformation applied for better model performance

#### ğŸ¢ Top Brands by Volume
1. **Dell** - Highest number of laptops
2. **Lenovo** - Second most common
3. **HP** - Third position

#### ğŸ’ Premium Brands by Price
1. **Razer** - Highest average price
2. **LG** - Premium positioning
3. **MSI** - Gaming-focused high prices
4. **Apple** - Consistently expensive
5. **Microsoft** - Surface lineup premium

#### ğŸ“± Laptop Types
- **Notebook**: Most common (40%+ of dataset)
- **Gaming**: Second most popular
- **Ultrabook**: Premium thin laptops
- **Workstation**: Most expensive category
- **Netbook**: Least common and cheapest

#### ğŸ–¥ï¸ Display Insights
- **Common Size**: 15.6 inches (most popular)
- **Touchscreen Premium**: 30-40% price increase
- **IPS Display**: 20-25% price premium
- **Resolution Impact**: Higher PPI correlates with higher price

#### âš¡ Hardware Correlations
- **RAM**: 8GB most common, strong positive correlation with price
- **Storage**: SSD shows stronger correlation than HDD
- **CPU**: Intel Core i7 commands highest prices
- **GPU**: Nvidia GPUs associated with premium pricing

## ğŸ¤– Model Building & Evaluation

### Algorithms Tested

#### Linear Models
1. **Linear Regression** - RÂ² Score: ~0.78
2. **Ridge Regression** (Î±=10) - RÂ² Score: ~0.78
3. **Lasso Regression** (Î±=0.001) - RÂ² Score: ~0.78

#### Distance-Based
4. **K-Nearest Neighbors** (k=3) - RÂ² Score: ~0.75

#### Tree-Based Models
5. **Decision Tree** (max_depth=8) - RÂ² Score: ~0.80
6. **Random Forest** (n=100, depth=15) - RÂ² Score: ~0.88
7. **Extra Trees** (n=100, depth=15) - RÂ² Score: ~0.88

#### Gradient Boosting
8. **AdaBoost** (n=15) - RÂ² Score: ~0.75
9. **Gradient Boosting** (n=500) - RÂ² Score: ~0.87
10. **XGBoost** (n=45, depth=5) - RÂ² Score: ~0.86

#### Kernel Methods
11. **Support Vector Regression** (RBF kernel) - RÂ² Score: ~0.80

#### Ensemble Methods
12. **Voting Regressor** - RÂ² Score: ~0.89 â­
13. **Stacking Regressor** - RÂ² Score: ~0.88

### ğŸ† Best Model Configuration

```python
# Voting Regressor (Best Performance)
Estimators:
â”œâ”€â”€ Random Forest (weight=5)
â”‚   â”œâ”€â”€ n_estimators: 350
â”‚   â”œâ”€â”€ max_depth: 15
â”‚   â”œâ”€â”€ max_samples: 0.5
â”‚   â””â”€â”€ max_features: 0.75
â”œâ”€â”€ Gradient Boosting (weight=1)
â”‚   â”œâ”€â”€ n_estimators: 100
â”‚   â””â”€â”€ max_features: 0.5
â”œâ”€â”€ XGBoost (weight=1)
â”‚   â”œâ”€â”€ n_estimators: 25
â”‚   â”œâ”€â”€ learning_rate: 0.3
â”‚   â””â”€â”€ max_depth: 5
â””â”€â”€ Extra Trees (weight=1)
    â”œâ”€â”€ n_estimators: 100
    â”œâ”€â”€ max_depth: 10
    â””â”€â”€ max_features: 0.75

Performance:
â”œâ”€â”€ RÂ² Score: 0.89
â”œâ”€â”€ MAE: 0.14
â””â”€â”€ Training Time: ~45 seconds
```

## ğŸ“ˆ Model Performance Metrics

<div align="center">
  
  | Model | RÂ² Score | MAE | Training Time |
  |:------|:--------:|:---:|:-------------:|
  | **Voting Regressor** â­ | **0.89** | **0.14** | **~45s** |
  | Stacking Regressor | 0.88 | 0.15 | ~50s |
  | Random Forest | 0.88 | 0.15 | ~30s |
  | Extra Trees | 0.88 | 0.15 | ~30s |
  | Gradient Boosting | 0.87 | 0.16 | ~40s |
  | XGBoost | 0.86 | 0.16 | ~25s |
  | SVR | 0.80 | 0.19 | ~60s |
  | Decision Tree | 0.80 | 0.18 | ~5s |
  | Linear Regression | 0.78 | 0.20 | ~2s |
  | Ridge Regression | 0.78 | 0.20 | ~2s |
  | Lasso Regression | 0.78 | 0.20 | ~2s |
  | KNN | 0.75 | 0.21 | ~10s |
  
</div>



## ğŸ”§ Configuration & Hyperparameters

### Model Hyperparameters

```python
# Random Forest (Primary Model)
n_estimators = 350          # Number of trees
max_depth = 15              # Maximum tree depth
max_samples = 0.5           # Bootstrap sample size
max_features = 0.75         # Features per split
random_state = 3            # Reproducibility

# Gradient Boosting
n_estimators = 100          # Boosting iterations
max_features = 0.5          # Feature sampling

# XGBoost
n_estimators = 25           # Boosting rounds
learning_rate = 0.3         # Step size shrinkage
max_depth = 5               # Tree complexity

# Extra Trees
n_estimators = 100          # Number of trees
max_depth = 10              # Maximum depth
max_features = 0.75         # Random feature selection
```

### Training Configuration

```python
# Train-Test Split
test_size = 0.2             # 20% holdout set
random_state = 42           # Reproducible splits

# Target Transformation
transform = np.log          # Log transformation
inverse = np.exp            # Exponential for predictions

# Preprocessing
encoding = OneHotEncoder    # Categorical encoding
drop_first = True           # Avoid dummy trap
sparse_output = False       # Dense matrices
```





## ğŸ¤ Contributing

Contributions are welcome to enhance the Laptop Price Predictor!

1. **ğŸ´ Fork the repository**

2. **ğŸŒŸ Create your feature branch**
   ```bash
   git checkout -b feature/ImprovedFeatureEngineering
   ```

3. **ğŸ’» Commit your changes**
   ```bash
   git commit -m 'Add GPU memory as additional feature'
   ```

4. **ğŸš€ Push to the branch**
   ```bash
   git push origin feature/ImprovedFeatureEngineering
   ```

5. **ğŸ“¬ Open a Pull Request**

### Development Guidelines

- Follow PEP 8 style guidelines for Python code
- Add comprehensive docstrings for new functions
- Include unit tests for new features
- Update documentation for changes
- Maintain backward compatibility with saved models
- Test predictions thoroughly before deployment

## ğŸ§ª Future Enhancements

### Planned Features

- **ğŸŒ Deep Learning Models** - Implement neural networks for comparison
- **ğŸ“Š Feature Selection** - Automated feature importance analysis
- **ğŸ”„ Real-time Data** - Scrape current laptop prices for continuous updates
- **ğŸ“± Mobile App** - React Native mobile application
- **ğŸ¨ Enhanced UI** - Interactive visualizations in Streamlit
- **ğŸŒ Multi-Currency** - Support for different currency predictions
- **ğŸ“ˆ Price Trends** - Historical price analysis and forecasting
- **ğŸ¤– AutoML** - Automated hyperparameter tuning with Optuna
- **â˜ï¸ Cloud Deployment** - AWS/GCP/Azure deployment guide
- **ğŸ“¦ Docker Container** - Containerized application for easy deployment

### Model Improvements

- **ğŸ¯ Target Encoding** - Experiment with target encoding for categorical features
- **ğŸ” Outlier Detection** - Implement robust outlier handling
- **ğŸ“Š Cross-Validation** - K-fold cross-validation for better evaluation
- **ğŸŒ³ Feature Interactions** - Polynomial features and interaction terms
- **âš¡ GPU Acceleration** - RAPIDS for faster training
- **ğŸ² Bayesian Optimization** - Hyperparameter optimization


## ğŸ“ Educational Value

This project demonstrates key concepts in:

- **Machine Learning**: Regression, ensemble methods, hyperparameter tuning
- **Data Science**: EDA, feature engineering, data visualization
- **Software Engineering**: Modular code, pipelines, deployment
- **Web Development**: Streamlit applications, user interfaces
- **Model Deployment**: Serialization, production pipelines
---
*Last updated: January 2026*




